# -*- coding: utf-8 -*-
"""Salinan dari coding fix 0.3

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q1V4r5cm9L9Kp9pXusliZFo8kZeRJH2u
"""

import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import numpy as np

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split #Library untuk memisahkan dataset menjadi train dan test
from sklearn.naive_bayes import CategoricalNB #Import Library untuk GaussianNaiveBayes
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score #Import Library untuk Confusion Matrix dan Accuracy (sejenis evaluasi)
from sklearn.metrics import classification_report #Import Library untuk TINGKAT_KEification Report (sejenis evaluasi)

"""# Persiapan Data"""

das = pd.read_csv('/content/DATA.csv')
print(das)

das.info()

#Tampilan Data 5 terakhir
das.tail(5)

"""# Cleanning Data"""

missing_values = das.isnull().sum()
print(missing_values)

"""# Data Selection

## Menjawab Rumusan Masalah
"""

#Menambahkan library untuk visualisasi data
import plotly.figure_factory as ff
import plotly.express as px

# Calculate count of each category
category_counts = das['KELAS_VEG'].value_counts()

# Create pie chart
fig = px.pie(category_counts, values=category_counts.values, names=category_counts.index, title="Melihat Data Kelas Vegetasi (Penutupan Lahan)")

# Show the pie chart
fig.show()

fig = px.bar(das['KELAS_VEG'].value_counts(),
        title ="Melihat Data Kelas Vegetasi")
fig.show()

# Calculate count of each category
category_counts = das['KELAS_LERENG'].value_counts()

# Create pie chart
fig = px.pie(category_counts, values=category_counts.values, names=category_counts.index, title="Melihat Data Kelas LERENG")

# Show the pie chart
fig.show()

fig = px.bar(das['KELAS_LERENG'].value_counts(),
        title ="Melihat Data Kelas Vegetasi")
fig.show()

fig = px.bar(das['KELAS_EROSI'].value_counts(),
        title ="Melihat Data Kelas Vegetasi")
fig.show()

# Calculate count of each category
category_counts = das['KELAS_EROSI'].value_counts()

# Create pie chart
fig = px.pie(category_counts, values=category_counts.values, names=category_counts.index, title="Melihat Data Kelas EROSI")

# Show the pie chart
fig.show()

# Calculate count of each category
category_counts = das['KRITIS'].value_counts()

# Create pie chart
fig = px.pie(category_counts, values=category_counts.values, names=category_counts.index, title="Melihat Data Kelas Vegetasi (Penutupan Lahan)")

# Show the pie chart
fig.show()

"""# Data Transformasi


"""

das.loc[das['KELAS_VEG'] == 'sangat baik', 'KELAS_VEG'] = 5
das.loc[das['KELAS_VEG'] == 'baik', 'KELAS_VEG'] = 4
das.loc[das['KELAS_VEG'] == 'sedang', 'KELAS_VEG'] = 3
das.loc[das['KELAS_VEG'] == 'buruk', 'KELAS_VEG'] = 2
das.loc[das['KELAS_VEG'] == 'sangat buruk', 'KELAS_VEG'] = 1

das.loc[das['KELAS_LERENG'] == 'datar', 'KELAS_LERENG'] = 5
das.loc[das['KELAS_LERENG'] == 'landai', 'KELAS_LERENG'] = 4
das.loc[das['KELAS_LERENG'] == 'agak curam', 'KELAS_LERENG'] = 3
das.loc[das['KELAS_LERENG'] == 'curam', 'KELAS_LERENG'] = 2
das.loc[das['KELAS_LERENG'] == 'sangat curam', 'KELAS_LERENG'] = 1


das.loc[das['KELAS_EROSI'] == 'sangat ringan', 'KELAS_EROSI'] = 0
das.loc[das['KELAS_EROSI'] == 'ringan', 'KELAS_EROSI'] = 1
das.loc[das['KELAS_EROSI'] == 'sedang', 'KELAS_EROSI'] = 2
das.loc[das['KELAS_EROSI'] == 'berat', 'KELAS_EROSI'] = 3
das.loc[das['KELAS_EROSI'] == 'sangat berat', 'KELAS_EROSI'] = 4

das.loc[das['KRITIS'] == 'Sangat Kritis', 'KRITIS'] = 5
das.loc[das['KRITIS'] == 'Kritis', 'KRITIS'] = 4
das.loc[das['KRITIS'] == 'Agak Kritis', 'KRITIS'] = 3
das.loc[das['KRITIS'] == 'Potensial Kritis', 'KRITIS'] = 2
das.loc[das['KRITIS'] == 'Tidak Kritis', 'KRITIS'] = 1

das = das.apply(pd.to_numeric, errors='coerce')
das

missing_values = das.isnull().sum()
print(missing_values)

# Variabel independen
x = das.drop(["KRITIS"], axis = 1)
x.head()

# Variabel dependen
y = das["KRITIS"]
y.head()

"""# Data Mining"""

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=0)

param_grid_nb = {
    'var_smoothing': np.logspace(0,-9, num=100)
}

from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import GridSearchCV

nbModel_grid = GridSearchCV(estimator=GaussianNB(), param_grid=param_grid_nb, verbose=1, cv=10, n_jobs=-1)
nbModel_grid.fit(x_train, y_train)
print(nbModel_grid.best_estimator_)

y_pred = nbModel_grid.predict(x_test)
print(y_pred)

accuracy = accuracy_score(y_test, y_pred)
print(accuracy_score(y_test, y_pred))

"""# Evaluasi Model dengan Confusion Matrix"""

# Menghitung akurasi klasifikasi
accuracy = accuracy_score(y_test, y_pred)

from sklearn.metrics import confusion_matrix
print(confusion_matrix(y_test, y_pred), "\t: is the confusion matrix")
#akurasi matriks
from sklearn.metrics import accuracy_score
print(accuracy_score(y_test, y_pred), "\t: is the accuracy score")

from sklearn.metrics import precision_score
# Menghitung precision score (multikelas)
precision_micro = precision_score(y_test, y_pred, average='micro')
precision_macro = precision_score(y_test, y_pred, average='macro')
precision_weighted = precision_score(y_test, y_pred, average='weighted')
print("Precision Score (Micro):", precision_micro)
print("Precision Score (Macro):", precision_macro)
print("Precision Score (Weighted):", precision_weighted)

from sklearn.metrics import recall_score
# Menghitung recall score (multikelas)
recall_micro = recall_score(y_test, y_pred, average='micro')
recall_macro = recall_score(y_test, y_pred, average='macro')
recall_weighted = recall_score(y_test, y_pred, average='weighted')
print("Recall Score (Micro):", recall_micro)
print("Recall Score (Macro):", recall_macro)
print("Recall Score (Weighted):", recall_weighted)


from sklearn.metrics import f1_score
# Menghitung f1 score (multikelas)
f1_micro = f1_score(y_test, y_pred, average='micro')
f1_macro = f1_score(y_test, y_pred, average='macro')
f1_weighted = f1_score(y_test, y_pred, average='weighted')
print("F1 Score (Micro):", f1_micro)
print("F1 Score (Macro):", f1_macro)
print("F1 Score (Weighted):", f1_weighted)

# Menghitung tingkat akurasi
accuracy_score(y_test, y_pred)

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))

# Calculate the confusion matrix
#
conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)
#
# Print the confusion matrix using Matplotlib
#
fig, ax = plt.subplots(figsize=(5, 5))
ax.matshow(conf_matrix, cmap=plt.cm.Oranges, alpha=0.3)
for i in range(conf_matrix.shape[0]):
    for j in range(conf_matrix.shape[1]):
        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')

plt.xlabel('Predictions', fontsize=18)
plt.ylabel('Actuals', fontsize=18)
plt.title('Confusion Matrix', fontsize=18)
plt.show()

"""# Visualisasi"""

sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix\nAccuracy: {:.2f}'.format(accuracy))
plt.show()

plt.figure(figsize=(8, 6))
sns.countplot(x=y_pred, palette='Blues')
plt.xlabel('Predicted Class')
plt.ylabel('Count')
plt.title('Prediction Results\nAccuracy: {:.2f}'.format(accuracy))

# Tambahkan keterangan jumlah pada setiap bar
for p in ax.patches:
    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', xytext=(0, 10), textcoords='offset points')

plt.show()

# Konversi hasil prediksi ke DataFrame
df_result = pd.DataFrame({'Actual Class': y_test, 'Predicted Class': y_pred})

# Simpan DataFrame ke file CSV
df_result.to_csv('hasil_prediksi.csv', index=False)

# Simpan X_test dan y_test ke dalam file CSV
x_test.to_csv('independen Baru.csv', index=False)
y_test.to_csv('dependen Baru.csv', index=False)